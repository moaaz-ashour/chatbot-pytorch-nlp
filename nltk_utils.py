import nltk

# 1
def tokenize(sentence):
    return nltk.word_tokenize(sentence)


# 2
def stem(word):
    pass


# 3
def bag_of_words(tokenized_sentence, all_words):
    pass
